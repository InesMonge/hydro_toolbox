{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "AIS_dir = \"/home/imonge/Documents/AIS/files\"\n",
    "output_dir = \"../../../../data/test_correlation/AIS/AIS.pkl\""
   ],
   "id": "4cfb3f2c6a183577",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#### Clean AIS dataset####",
   "id": "2e4f8940a4b76e1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Retrieve csv files\n",
    "csv_files = glob.glob(os.path.join(AIS_dir, \"*.csv\"))\n",
    "dataframes = []\n",
    "\n",
    "# Load each file as a DataFrame\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file, parse_dates=[\"datetime\"])\n",
    "        dataframes.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la lecture de {file} : {e}\")\n",
    "\n",
    "# Merge csv\n",
    "AIS = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Standardise columns names\n",
    "AIS.rename(columns=lambda x: x.strip().lower(), inplace=True)\n",
    "\n",
    "# Convert 0 for width and length and draught in NaN\n",
    "AIS[\"length\"] = AIS[\"length\"].replace(0, pd.NA)\n",
    "AIS[\"width\"] = AIS[\"width\"].replace(0, pd.NA)\n",
    "AIS[\"draught\"] = AIS[\"draught\"].replace(0, pd.NA)"
   ],
   "id": "ddcaaa1baa8ad8f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Remove incoherent data\n",
    "# Datetime duplicates\n",
    "AIS.drop_duplicates(subset=[\"mmsi\", \"datetime\"], inplace=True)\n",
    "\n",
    "# Remove several lengths or widths for one mmsi\n",
    "mmsi_multi_length = AIS.groupby(\"mmsi\")[\"length\"].nunique()\n",
    "mmsi_multi_width = AIS.groupby(\"mmsi\")[\"width\"].nunique()\n",
    "\n",
    "mmsi_invalid = mmsi_multi_length[mmsi_multi_length > 1].index.union(\n",
    "    mmsi_multi_width[mmsi_multi_width > 1].index\n",
    ")\n",
    "\n",
    "AIS = AIS[~AIS[\"mmsi\"].isin(mmsi_invalid)].copy()"
   ],
   "id": "549df128fae5751",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## NaN values\n",
    "# Replace NaN in length, width and draught columns by the known values for each mmsi\n",
    "mmsi_length_map = AIS.groupby(\"mmsi\")[\"length\"].first()\n",
    "mmsi_width_map = AIS.groupby(\"mmsi\")[\"width\"].first()\n",
    "mmsi_draught_map = AIS.groupby(\"mmsi\")[\"draught\"].first()\n",
    "\n",
    "AIS[\"length\"] = AIS.apply(\n",
    "    lambda row: mmsi_length_map[row[\"mmsi\"]] if pd.isna(row[\"length\"]) else row[\"length\"], axis=1\n",
    ")\n",
    "AIS[\"width\"] = AIS.apply(\n",
    "    lambda row: mmsi_width_map[row[\"mmsi\"]] if pd.isna(row[\"width\"]) else row[\"width\"], axis=1\n",
    ")\n",
    "AIS[\"draught\"] = AIS.apply(\n",
    "    lambda row: mmsi_draught_map[row[\"mmsi\"]] if pd.isna(row[\"draught\"]) else row[\"draught\"], axis=1\n",
    ")\n",
    "\n",
    "# Remove mmsi with no values for length or width\n",
    "mmsi_with_length = AIS.groupby(\"mmsi\")[\"length\"].apply(lambda x: x.notna().any())\n",
    "mmsi_with_width = AIS.groupby(\"mmsi\")[\"width\"].apply(lambda x: x.notna().any())\n",
    "valid_mmsi = mmsi_with_length & mmsi_with_width\n",
    "AIS = AIS[AIS[\"mmsi\"].isin(valid_mmsi[valid_mmsi].index)].copy()"
   ],
   "id": "6c10b3b550f0dbdb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Remove speed outliers\n",
    "# Filter sog above 3 knots\n",
    "df_speed = AIS[(~AIS[\"sog\"].isna()) & (AIS[\"sog\"] > 3)]\n",
    "\n",
    "# Calculate 99th percentile\n",
    "threshold_99 = df_speed[\"sog\"].quantile(0.999)\n",
    "\n",
    "# Identify outliers\n",
    "outliers = df_speed[df_speed[\"sog\"] > threshold_99]\n",
    "\n",
    "AIS = AIS[~AIS.index.isin(outliers.index)].copy()"
   ],
   "id": "2ba77fb52368dfc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Switch \"lat\" and \"lon\" columns names\n",
    "AIS[\"lat\"], AIS[\"lon\"] = AIS[\"lon\"].copy(), AIS[\"lat\"].copy()"
   ],
   "id": "c61cf3b76ddc9a81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ## Save dataset as pkl\n",
    "# # Sort by datetime\n",
    "# AIS.sort_values(by=\"datetime\", inplace=True)\n",
    "#\n",
    "# # Reset index\n",
    "# AIS.reset_index(drop=True, inplace=True)\n",
    "#\n",
    "# # Save dataframe as pkl\n",
    "# AIS.to_pickle(output_dir)"
   ],
   "id": "d8fc7e23c5bbd13c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#### Data Verification ####",
   "id": "d4c11e38c9040a45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# AIS dataframe verification\n",
    "# Datetime duplicates\n",
    "duplicate_mmsi_datetime = AIS.duplicated(subset=[\"mmsi\", \"datetime\"]).sum()\n",
    "print(f\"Duplicates mmsi + datetime: {duplicate_mmsi_datetime}\")\n",
    "\n",
    "# Mmsi with different length / width\n",
    "nb_multi_length = AIS.groupby(\"mmsi\")[\"length\"].nunique()\n",
    "nb_multi_length = nb_multi_length[nb_multi_length > 1].index\n",
    "\n",
    "nb_multi_width = AIS.groupby(\"mmsi\")[\"width\"].nunique()\n",
    "nb_multi_width = nb_multi_width[nb_multi_width > 1].index\n",
    "\n",
    "nb_multi_both = nb_multi_length.intersection(nb_multi_width)\n",
    "\n",
    "print(f\"mmsi with multiple length: {len(nb_multi_length)}\")\n",
    "print(f\"mmsi with multiple width: {len(nb_multi_width)}\")\n",
    "print(f\"mmsi with multiple both: {len(nb_multi_both)}\")\n"
   ],
   "id": "3fdb993d9ec0b717",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Nan values\n",
    "# Number of nan values\n",
    "total_nan =  AIS.isna().sum().sum()\n",
    "print(f\"Total nan values: {total_nan}\")\n",
    "\n",
    "# Columns corresponding\n",
    "na_per_column = AIS.isna().sum()\n",
    "print(\"Columns corresponding\")\n",
    "print(f\"{na_per_column}\")\n",
    "\n",
    "# Number of boats without dimension\n",
    "mmsi_no_dim = AIS[AIS[\"length\"].isna() & AIS[\"width\"].isna()][\"mmsi\"].unique()\n",
    "print(f\"Number of boats without dimension: {len(mmsi_no_dim)}\")"
   ],
   "id": "c1c8958b6811f5c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "length_counts = AIS.groupby(\"mmsi\")[\"length\"].nunique()\n",
    "width_counts = AIS.groupby(\"mmsi\")[\"width\"].nunique()\n",
    "\n",
    "mmsi_valid = length_counts[(length_counts == 1) & (width_counts == 1)].index\n",
    "mmsi_invalid = length_counts[(length_counts > 1) & (width_counts > 1)].index\n",
    "mmsi_missing = length_counts[length_counts == 0].index\n",
    "tot_boat = AIS[\"mmsi\"].nunique()\n",
    "tot_mmsi = len(mmsi_valid) + len(mmsi_invalid) + len(mmsi_missing)\n",
    "\n",
    "print(f\"mmsi avec une seule valeur de longueur et largeur: {len(mmsi_valid)}\")\n",
    "print(f\"mmsi avec plusieurs valeurs de longueurs ou largeurs: {len(mmsi_invalid)}\")\n",
    "print(f\"mmsi sans valeurs renseign√©es: {len(mmsi_missing)}\")\n",
    "print(f\"Total number of boats (mmsi): {tot_boat}\")"
   ],
   "id": "9493114bbae66394",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f\"Maximum speed : {AIS[\"sog\"].max()} knots\")",
   "id": "aab1814449c1d6ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T07:27:36.491445Z",
     "start_time": "2025-07-15T07:27:36.440953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Latitude (min, max):\", AIS[\"lat\"].min(), AIS[\"lat\"].max())\n",
    "print(\"Longitude (min, max):\", AIS[\"lon\"].min(), AIS[\"lon\"].max())"
   ],
   "id": "dac240c88b9ba992",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latitude (min, max): 43.99990167 47.00002667\n",
      "Longitude (min, max): -14.000015 -10.99997667\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check for lon/lat outliers\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(AIS[\"lat\"], bins = 100, color = \"skyblue\", edgecolor=\"black\")\n",
    "plt.title(\"Latitudes distribution\")\n",
    "plt.xlabel(\"Latitude\")\n",
    "plt.ylabel(\"Number of occurrences\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#plt.savefig(\"lat_histo.png\")\n",
    "\n",
    "# Plot longitude histogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(AIS[\"lon\"], bins = 100, color = \"salmon\", edgecolor=\"black\")\n",
    "plt.title(\"Longitudes distribution\")\n",
    "plt.xlabel(\"Longitudes\")\n",
    "plt.ylabel(\"Number of occurrences\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#plt.savefig(\"lon_histo.png\")"
   ],
   "id": "a76d63ab54e8412c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
